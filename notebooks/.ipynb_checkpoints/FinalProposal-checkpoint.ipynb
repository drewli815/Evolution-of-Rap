{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In our project, we wanted to determine if rap music has changed over the last few decades. We've gathered data from Billboard's Hot Weekly charts containing 28000+ songs and trimmed it down to approximately 3800 rap songs. From this analysis, we found INCLUDE FINAL ANALYSIS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permissions, Names/PIDs, and Githubs\n",
    "\n",
    "Permission Consensus: \n",
    "- [X] YES - make available\n",
    "- [] NO - keep private\n",
    "\n",
    "Names/PIDS - Githubs\n",
    "- Andrew Li/A15664397 - drewli815\n",
    "- Jialin Shan/A13826733 - j5shan\n",
    "- Sameer Ahmed/A16094697 - the3L3M3NT\n",
    "- Lacey Umamoto/A15726197 - lumamoto\n",
    "- Rickesh Khilnani/A15481687 - rick10101221"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question\n",
    "\n",
    "In the past 60 years, how has rap music's lyrics and overarching themes evolved into the repetitive, monotonous genre it has become today? Can we form the conclusion that the main topics in rap have not changed after performing an analysis of the Billboard's number-one rap singles for each decade ranging from the 1960s to the 2010s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Research\n",
    "\n",
    "- We realized that contemporary music is repetitive because a lot of songs have a repeating theme in the form of either lyrics or the type of beat. This is very evident in contemporary rap. A lot of rap songs cover the same topics: guns, liquor, drugs, money, sex, getting famous, being rich, etc. But one has to consider the type of beat that rap music uses. It's quite hard to tell the difference from one rap song to another if you were to just listen to the instrumental versions. The dull and repetitive nature of rap comes from exactly this.\n",
    "\n",
    "- Therefore, in order to measure the monotony, we will be accumulating data from thousands of songs ranging from the 1960s - 2010s and performing an analysis on their overarching themes by using TF-IDF on the lyrics of all 3800 rap songs and looking at the differences in unique and common words. The American rap genre will be considered to have evolved from innovative to monotonous if rap music from the 1960s covers a range of topics other than the ones listed above as opposed to contemporary rap. In the 20th century, rap music was much more impactful and many pieces pushed to make public change. Nowadays, that isn't always the case. At least, not as often.\n",
    "\n",
    "- The overarching theme of each song will be the most repeated keyword (after removing common english words), word counts will be some sort of map or Counter that associates frequencies with words. We will primarily be using Genius' API to gather lyrics.\n",
    "\n",
    "- We believe there should be a change in the music industry. Today's music industry makes music that is solely for 'instant gratification' and is completely shallow. (Refer to the second resource in the *Past Studies* section below)\n",
    "\n",
    "- We all listen to and enjoy music.\n",
    "\n",
    "- Past Studies\n",
    "    - There is an [article](https://www.rapanalysis.com/2015/08/the-23-most-repetitive-rappers/) that has illustrated the 23 most repetitive rappers, the highest being Will.I.Am and Kid Cudi. The analysis looked at how often certain keywords were said, but only compared it from artist to artist.\n",
    "    - In this [tiny piece](https://roundup.brophyprep.org/index.php/2012/03/popular-hip-hop-music-praises-shallow-superficial-decadence/) (written 9 years ago), talks about how music used to promote good values, but in the 21st century, it has completely shifted to a shallow and superficial genre. He adds a quote from one of Drake's songs that only focuses on the aspects of life that are instantly gratifying: getting money, drinking, and smoking, namely.\n",
    "\n",
    "- References (include links):\n",
    "    - [Billboard's Hot Weekly Charts](https://data.world/kcmillersean/billboard-hot-100-1958-2017) \n",
    "    - [Genius API](https://docs.genius.com/)\n",
    "    - [Lyrics-Extractor Package for Python](https://pypi.org/project/lyrics-extractor/) (canceled)\n",
    "        - This can be used for retrieving and cross-referencing lyrics for data validity. Uses Genius' API\n",
    "    - [Language Detector](https://pypi.org/project/langdetect/) \n",
    "    - [Spotify's Web API](https://spotipy.readthedocs.io/en/2.16.0/) (canceled)\n",
    "    - [Spotify](https://www.spotify.com/us/) (canceled)\n",
    "    - [MusixMatch API for lyrics](https://developer.musixmatch.com/) (canceled)  \n",
    "    - Google Sheets to store accumulated data for each decade (at the end of project for reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "After conducting a thorough analysis of music repetition, we believe that there exists a trend in contemporary rap music. This trend will be evident after comparing the overarching themes of rap music for each decade. We expect that, through these observations, we will find a repetitive trend because contemporary rap music focuses on the same topics for each song whereas older rap music used to cover a diverse range of topics so that each song and artist was refreshing and unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- What variables: song name, song title, song year, lyrics for song (dataframe 1), and keywords with their respective counts (dataframe 2). Dataframe 1 will contain the data for all 3800 songs and dataframe 2 will divide the respective counts for each decade in separate columns.\n",
    "\n",
    "- How many observations: Varies depending on how many songs are on each Billboard for each decade. We will have to find a way to normalize our data so that the number of songs for each decade are the same. In total, however, we will trim a dataset of 28000 songs down to 3800 which are specifically rap songs.\n",
    "\n",
    "- Who/what/how would these data be collected: \n",
    "    - Who\n",
    "        - Each week, teams will be formed and tasks will be assigned to each subteam so that we can achieve maximum efficiency in data collection and analysis. Everyone's time would not be spent completing the same task and we will finish the project quicker. For example, team A may be tasked with data collection while team B will be in charge of parsing through it for cleaning (removing stop words, stemming, etc.)\n",
    "    - What\n",
    "        - At first, we will be collecting song names, artists, and the date at which each song reached number one. Then, using lyrics extractor, we will find the lyrics for each song. At this point, this is all the data collection we need. Additional steps include parsing through the data with TF-IDF, removing common English words, and developing an extra dataframe for holding word counts for each decade..\n",
    "    - How\n",
    "        - First using Billboard's number-one songs and exporting as a csv file. Then, we will read it in as a dataframe so that we can then iterate through each song title and run the lyrics extractor package on it. Then we will simply store each lyric body in the dataframe, remove any common English words from each lyric body, and iterate through all of the lyrics and associate counts with each word.\n",
    "\n",
    "- How would these data be stored/organized: Pandas dataframe which will be used for building visualizations. Again, the data will be organized as two separate data frames for processing (see above)\n",
    "\n",
    "- What kind of songs are you collecting: We will be collecting Billboard's number-one songs for each decade from the 1960s to the 2010s. The number of songs for each decade varies, so we will have to establish a numerical bottleneck.\n",
    "\n",
    "- Dataset 1:\n",
    "    - Dataset Name: Billboard Hot Weekly Charts\n",
    "    - [Link to the dataset](https://data.world/kcmillersean/billboard-hot-100-1958-2017)\n",
    "    - Number of total observations: 28500\n",
    "    - Truncated observation count for specifically 'rap': 3850\n",
    "\n",
    "- Notes\n",
    "    - We will ONLY be looking at rap songs for this analysis. This is purely from Billboard's Hot Weekly charts so that we get a rough estimate of what the most popular songs are for each decade. Therefore, this may not capture ALL topics in rap over the last 6 decades.\n",
    "    - Because we have a method of retrieving all of the lyrics for each song in our data set, we know that we will have more than 25000+ words and 3800+ songs to parse through, which is more than enough to model the overarching themes for each decade for comparison with one another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from autocorrect import Speller\n",
    "from collections import Counter \n",
    "import spacy\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Cut down 28000 observations to 3400 specifically pertaining to rap, merging columns to get a single ID, dropping identical songs, and creating a final dataframe to hold the lyrics of every song.\n",
    "\n",
    "Read in the Billboard's Hot weekly charts and their audio features retrieved from Spotify Web API (`df`), and when each correponding song has hit Billboard (`df_hotstuff`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_excel(\"../data/Hot 100 Audio Features.xlsx\"))\n",
    "df_hotstuff = pd.DataFrame(pd.read_csv(\"../data/Hot Stuff.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting Down Data\n",
    "\n",
    "Here, we are dropping any obserations without any genre specified, assuming that they were not considered rap songs because they weren't explicitly tagged with it. This **ALSO** excludes songs without a rap tag entirely, including 'Hip Hop' songs (hip hop songs have an explcit 'hip hop' tag). Therefore, we are solely focusing on rap music in the entire data set. \n",
    "\n",
    "`df_rap` will contain observations that have the 'rap' genre in their spotify_genre's list. Lastly, we drop duplicates that may have emerged through the data collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop songs without genres\n",
    "df.dropna(subset=['spotify_genre'], inplace=True)\n",
    "\n",
    "# get songs with rap genre\n",
    "df_rap = pd.DataFrame()\n",
    "for index, row in df.iterrows():\n",
    "    genres = df.spotify_genre.squeeze()[index]\n",
    "    if 'rap' in genres:\n",
    "        df_rap = df_rap.append(row)\n",
    "\n",
    "# drop duplicate songs (songs with same songID)\n",
    "df_rap = df_rap.drop_duplicates(subset=['SongID'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Dataframes\n",
    "\n",
    "Merging both datasets so we can work with one dataframe. We also drop any duplicates as a result of the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df_rap and df_hotstuff to get weekID\n",
    "df_merge = pd.merge(df_rap, df_hotstuff, how='left')\n",
    "\n",
    "# drop songs with no weekID\n",
    "df_merge.dropna(subset=['WeekID'], inplace=True)\n",
    "\n",
    "# drop duplicate songs (songs with same songID)\n",
    "df_merge = df_merge.drop_duplicates(subset=['SongID'],keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Years\n",
    "\n",
    "For our data analysis, we only need to keep the `Year`, `Performer` and `Song` title for each observation in order to retrieve the lyrics. Since we only considering data on a per decade basis, we can abstract away the precision given by months and days, leaving only years. We store this in `df_final`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get years\n",
    "years = []\n",
    "for index, row in df_merge.iterrows():\n",
    "    weekID = df_merge.WeekID[index]\n",
    "    year = datetime.strptime(weekID, \"%m/%d/%Y\").year\n",
    "    years.append(year)\n",
    "df_merge['Year'] = years\n",
    "\n",
    "# get final dataframe with year, performer, and song\n",
    "df_final = df_merge[['Year', 'Performer', 'Song']]\n",
    "df_final = df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function\n",
    "\n",
    "Strips accents from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip accents from text\n",
    "# ex. beyoncÃ© --> beyonce\n",
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make API call to get page to scrape from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(song_title, artist_name):\n",
    "    # print(\"Searching for: \", song_title, \"-\", artist_name)\n",
    "    \n",
    "    # get song title and artist\n",
    "    # convert to lowercase, remove non-alphanumeric characters\n",
    "    \n",
    "    title = re.sub(r'[^a-zA-Z0-9-$() ]', '', song_title.lower())\n",
    "    #print(\"Title:\", title)\n",
    "    \n",
    "    title_simple = re.sub(r'[^a-zA-Z0-9- ]', '', title)\n",
    "    #print(title_simple)\n",
    "    \n",
    "    # title with no parentheses\n",
    "    title_noparen = re.sub(r'\\([^)]*\\)', '', title)\n",
    "    #print(\"Title no paren:\", title_noparen)\n",
    "    \n",
    "    # replace dollar signs with s's\n",
    "    title_nodollar = title.replace(\"$\", \"s\")\n",
    "    #print(title_nodollar)\n",
    "    \n",
    "    artist = re.sub(r'[^a-zA-Z0-9-$() ]', '', artist_name.lower())\n",
    "    #print(\"Artist:\", artist)\n",
    "    \n",
    "    artist_simple = re.sub(r'[^a-zA-Z0-9- ]', '', artist)\n",
    "    #print(artist_simple)\n",
    "    \n",
    "    artist_nodollar = artist.replace(\"$\", \"s\")\n",
    "    #print(artist_nodollar)\n",
    "    \n",
    "    artist_split = artist.split()\n",
    "    #print(\"Artist Split:\", artist_split)\n",
    "    \n",
    "    # main artist\n",
    "    if 'featuring' in artist:\n",
    "        artist_nofeat = artist.split('featuring')[0]\n",
    "    elif ',' in artist:\n",
    "        artist_nofeat = artist.split(',')[0]\n",
    "    else:\n",
    "        artist_nofeat = artist \n",
    "    #print(\"Artist No Feat:\", artist_nofeat)\n",
    "    \n",
    "    # set up request\n",
    "    headers = {'Authorization': 'Bearer ' + 'zZ6YtjOlYsm1o5Me_vIO6MczexIf6k5PGlgiMHi4aO6bnZmsyVdG7J7YQ0VXIOHE'}\n",
    "    data = {'q': title_noparen + ' ' + artist_nofeat}\n",
    "    base_url = 'https://api.genius.com'\n",
    "    search_url = base_url + '/search'\n",
    "    \n",
    "    current_page = 1 # page number of results\n",
    "    next_page = True\n",
    "    \n",
    "    while next_page:\n",
    "        params = {'page': current_page} # set page number\n",
    "        response = requests.get(search_url, data=data, headers=headers, params=params)\n",
    "        d = response.json()\n",
    "        page_hits = d['response']['hits']\n",
    "        \n",
    "        # if there are hits on the page\n",
    "        if page_hits:\n",
    "            # go through all hits\n",
    "            for hit in page_hits:\n",
    "                res = hit['result']\n",
    "                \n",
    "                # name of primary artist\n",
    "                name = res['primary_artist']['name']\n",
    "                name = strip_accents(name)\n",
    "                name = re.sub(r'[^a-zA-Z0-9- ]', '', name.lower())\n",
    "                #print(\"Name:\",name)\n",
    "                \n",
    "                full_title = res['full_title']\n",
    "                full_title = strip_accents(full_title)\n",
    "                # convert full_title to lowercase and remove non-alphanumeric characters\n",
    "                full_title = re.sub(r'[^a-zA-Z0-9- ]', '', full_title.lower())\n",
    "                #print(\"Full Title:\", full_title)\n",
    "                \n",
    "                if (\n",
    "                    # 'lyrics' substring is in url\n",
    "                    'lyrics' in res['url'] and\n",
    "                     # song title (w/ or w/o parentheses) is in full title\n",
    "                    (title in full_title or \n",
    "                     title_noparen in full_title or\n",
    "                     title_nodollar in full_title or\n",
    "                     title_simple in full_title\n",
    "                    ) and\n",
    "                    # 1st or 2nd word in artist is in full title or \n",
    "                    # main artist (no features) is in full title or name\n",
    "                    (artist_nofeat in full_title or \n",
    "                     artist_nofeat in name or\n",
    "                     artist_split[0] in full_title or\n",
    "                     (len(artist_split) > 1 and artist_split[1] in full_title) or\n",
    "                     artist_nodollar in full_title or\n",
    "                     artist_nodollar in name or\n",
    "                     artist_simple in full_title or\n",
    "                     artist_simple in name\n",
    "                    ) and\n",
    "                    # 1st or 2nd word in artist is in name from response\n",
    "                    (artist_split[0] in name or\n",
    "                     (len(artist_split) > 1 and artist_split[1] in name)\n",
    "                    ) and\n",
    "                    # song is not a translation\n",
    "                    'espanol' not in full_title and\n",
    "                    'nederlandse' not in full_title and\n",
    "                    'polskie' not in full_title and\n",
    "                    'portugues' not in full_title and\n",
    "                    'francaise' not in full_title and\n",
    "                    'deutsche' not in full_title and\n",
    "                    'oversttelse' not in full_title and\n",
    "                    'traduzione' not in full_title and\n",
    "                    'ceviri' not in full_title and\n",
    "                    'translation' not in full_title and\n",
    "                    # song is not a review by rap critic\n",
    "                    'rap critic' not in full_title and\n",
    "                    # song is not instrumental\n",
    "                    'instrumental' not in full_title and\n",
    "                    # song is not a parody\n",
    "                    'parody' not in full_title\n",
    "                ):\n",
    "                    url = res['url']\n",
    "                    # print(\"URL found: \", url)\n",
    "                    return url\n",
    "                    \n",
    "            # increment current_page value for next loop\n",
    "            current_page += 1\n",
    "            # print(\"Finished scraping page {}\".format(current_page))\n",
    "            \n",
    "            # if lyrics not on first 10 pages, stop\n",
    "            if (current_page == 10):\n",
    "                next_page = False\n",
    "        else:\n",
    "            # if page_hits is empty, stop\n",
    "            next_page = False\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape page for lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(song_title, artist_name):\n",
    "    url = get_url(song_title, artist_name)\n",
    "    if url == 0:\n",
    "        print(\"Lyrics not found for\", song_title, \"-\", artist_name)\n",
    "        return np.NaN\n",
    "    else:\n",
    "        page = requests.get(url)\n",
    "        html = BeautifulSoup(page.text, 'html.parser')\n",
    "        lyrics = html.find('div', class_='lyrics').get_text()\n",
    "        return lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get lyrics\n",
    "\n",
    "Uses all three helper functions defined above to retrieve the lyrics for all rap songs. For quick access (and to avoid having to rerun this cell at all), we store the final dataframe in `Hot100DataWithNanLyrics.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "0.28% done\n",
      "0.56% done\n",
      "0.83% done\n",
      "1.11% done\n",
      "1.39% done\n",
      "1.67% done\n",
      "1.95% done\n",
      "2.23% done\n",
      "2.5% done\n",
      "Lyrics not found for 223's - YNW Melly & 9lokknine\n",
      "2.78% done\n",
      "3.06% done\n",
      "3.34% done\n",
      "3.62% done\n",
      "3.9% done\n",
      "4.17% done\n",
      "4.45% done\n",
      "4.73% done\n",
      "5.01% done\n",
      "5.29% done\n",
      "Lyrics not found for How About Now - Drake\n",
      "5.56% done\n",
      "5.84% done\n",
      "6.12% done\n",
      "6.4% done\n",
      "Lyrics not found for Apes**t - The Carters\n",
      "6.68% done\n",
      "6.96% done\n",
      "Lyrics not found for Dear Mama/Old School - 2Pac\n",
      "7.23% done\n",
      "7.51% done\n",
      "Lyrics not found for Move B***h - Ludacris Featuring Mystikal & Infamous 2.0\n",
      "7.79% done\n",
      "Lyrics not found for From Her Mama (Mama Got A**) - Juvenile\n",
      "Lyrics not found for Envy/Firewater - Fat Joe\n",
      "8.07% done\n",
      "8.35% done\n",
      "8.63% done\n",
      "8.9% done\n",
      "9.18% done\n",
      "9.46% done\n",
      "Lyrics not found for Sexy Lady - Yung Berg Featuring Junior\n",
      "9.74% done\n",
      "10.02% done\n",
      "10.29% done\n",
      "Lyrics not found for She's Mine Pt.1 - J. Cole\n",
      "10.57% done\n",
      "10.85% done\n",
      "11.13% done\n",
      "11.41% done\n",
      "Lyrics not found for 3am - Eminem\n",
      "11.69% done\n",
      "11.96% done\n",
      "12.24% done\n",
      "12.52% done\n",
      "12.8% done\n",
      "13.08% done\n",
      "Lyrics not found for All The Things (Your Man Won't Do) (From \"Don't Be A Menace...\") - Joe\n",
      "13.36% done\n",
      "Lyrics not found for Big Poppa/Warning - The Notorious B.I.G.\n",
      "13.63% done\n",
      "13.91% done\n",
      "Lyrics not found for Friends/Five Minutes Of Funk - Whodini\n",
      "14.19% done\n",
      "14.47% done\n",
      "14.75% done\n",
      "Lyrics not found for Dr. Jon (The Medicine Man) - Jon & Robin And The In Crowd\n",
      "15.03% done\n",
      "Lyrics not found for Oh No - Mos Def & Pharoahe Monch Featuring Nate Dogg\n",
      "15.3% done\n",
      "Lyrics not found for More Bounce To The Ounce Part I - Zapp\n",
      "15.58% done\n",
      "Lyrics not found for Juicy/Unbelievable - The Notorious B.I.G.\n",
      "15.86% done\n",
      "16.14% done\n",
      "Lyrics not found for It's All The Way Live (Now) (From \"Eddie\") - Coolio\n",
      "16.42% done\n",
      "Lyrics not found for (Hot S**t) Country Grammar - Nelly\n",
      "16.69% done\n",
      "16.97% done\n",
      "Lyrics not found for Do That... - Baby Featuring P. Diddy\n",
      "17.25% done\n",
      "Lyrics not found for Breathe, Stretch, Shake - Mase Featuring P. Diddy\n",
      "Lyrics not found for Rich As F**k - Lil Wayne Featuring 2 Chainz\n",
      "Lyrics not found for SuperThug (What What) - Noreaga\n",
      "17.53% done\n",
      "17.81% done\n",
      "18.09% done\n",
      "18.36% done\n",
      "Lyrics not found for Dope N****z - Lil Wayne Featuring Snoop Dogg\n",
      "18.64% done\n",
      "18.92% done\n",
      "Lyrics not found for One More Chance/Stay With Me - The Notorious B.I.G.\n",
      "19.2% done\n",
      "19.48% done\n",
      "19.76% done\n",
      "20.03% done\n",
      "Lyrics not found for Louie, Louie - Fat Boys\n",
      "Lyrics not found for Here's A Heart - The Diplomats\n",
      "20.31% done\n",
      "20.59% done\n",
      "20.87% done\n",
      "Lyrics not found for Magic Wand - Don & Juan\n",
      "21.15% done\n",
      "21.42% done\n",
      "21.7% done\n",
      "21.98% done\n",
      "Lyrics not found for Armed And Dangerous - Juice WRLD\n",
      "22.26% done\n",
      "22.54% done\n",
      "22.82% done\n",
      "23.09% done\n",
      "Lyrics not found for Before You Walk Out Of My Life/Like This And Like That - Monica\n",
      "23.37% done\n",
      "Lyrics not found for Bigger > You - 2 Chainz, Drake & Quavo\n",
      "23.65% done\n",
      "23.93% done\n",
      "24.21% done\n",
      "24.49% done\n",
      "Lyrics not found for 9 AM In Dallas - Drake\n",
      "24.76% done\n",
      "25.04% done\n",
      "25.32% done\n",
      "25.6% done\n",
      "25.88% done\n",
      "Lyrics not found for Do It Again A Little Bit Slower - Jon & Robin And The In Crowd\n",
      "26.15% done\n",
      "26.43% done\n",
      "Lyrics not found for My Sh*t - A Boogie Wit da Hoodie\n",
      "Lyrics not found for F*ckwithmeyouknowigotit - Jay Z Featuring Rick Ross\n",
      "26.71% done\n",
      "Lyrics not found for No BS - Chris Brown\n",
      "26.99% done\n",
      "27.27% done\n",
      "Lyrics not found for Love's Made A Fool Of You - Cochise\n",
      "27.55% done\n",
      "27.82% done\n",
      "28.1% done\n",
      "28.38% done\n",
      "28.66% done\n",
      "Lyrics not found for Pills And Automobiles - Chris Brown Featuring Yo Gotti, A Boogie Wit da Hoodie & Kodak Black\n",
      "28.94% done\n",
      "29.22% done\n",
      "Lyrics not found for The Business - Yung Berg Featuring Casha\n",
      "Lyrics not found for Ain't No Nigga/Dead Presidents - Jay-Z Featuring Foxxy Brown\n",
      "29.49% done\n",
      "29.77% done\n",
      "30.05% done\n",
      "30.33% done\n",
      "30.61% done\n",
      "30.88% done\n",
      "31.16% done\n",
      "31.44% done\n",
      "Lyrics not found for It Ain't My Fault 1 & 2 - Silkk The Shocker Featuring Mystikal\n",
      "31.72% done\n",
      "32.0% done\n",
      "32.28% done\n",
      "32.55% done\n",
      "Lyrics not found for Old Fashioned Boy (You're The One) - Stallion\n",
      "Lyrics not found for I Never Seen A Man Cry (aka I Seen A Man Die) - Scarface\n",
      "32.83% done\n",
      "33.11% done\n",
      "33.39% done\n",
      "33.67% done\n",
      "33.95% done\n",
      "34.22% done\n",
      "34.5% done\n",
      "34.78% done\n",
      "35.06% done\n",
      "Lyrics not found for Lovers And Friends - Lil Jon & The East Side Boyz Featuring Usher & Ludacris\n",
      "35.34% done\n",
      "Lyrics not found for Lookin' At Me - Mase Featuring Puff Daddy\n",
      "35.61% done\n",
      "Lyrics not found for I'm Going In - Drake Featuring Lil Wayne & Young Jeezy\n",
      "35.89% done\n",
      "Lyrics not found for F**k Love - XXXTentacion Featuring Trippie Redd\n",
      "36.17% done\n",
      "36.45% done\n",
      "36.73% done\n",
      "37.01% done\n",
      "37.28% done\n",
      "37.56% done\n",
      "37.84% done\n",
      "38.12% done\n",
      "38.4% done\n",
      "38.68% done\n",
      "Lyrics not found for I Got The Hook Up! - Master P Featuring Sons Of Funk\n",
      "38.95% done\n",
      "39.23% done\n",
      "Lyrics not found for Funky Cold Medina - Tone-Loc\n",
      "39.51% done\n",
      "Lyrics not found for Ganja Burns - Nicki Minaj\n",
      "39.79% done\n",
      "40.07% done\n",
      "40.35% done\n",
      "40.62% done\n",
      "Lyrics not found for How I Could Just Kill A Man/The Phuncky Feel One - Cypress Hill\n",
      "40.9% done\n",
      "Lyrics not found for Move That Doh - Future Featuring Pharrell, Pusha T & Casino\n",
      "41.18% done\n",
      "Lyrics not found for I'm A King - P$C Featuring T.I. & Lil Scrappy\n",
      "Lyrics not found for Memories Back Then - Hustle Gang Featuring T.I., B.o.B, Kendrick Lamar & Kris Stephens\n",
      "41.46% done\n",
      "41.74% done\n",
      "42.01% done\n",
      "42.29% done\n",
      "42.57% done\n",
      "Lyrics not found for Jigga My N**** - JAY-Z\n",
      "42.85% done\n",
      "Lyrics not found for I Wanna Get With U - Guy\n",
      "43.13% done\n",
      "Lyrics not found for Incarcerated Scarfaces/Ice Cream - Raekwon\n",
      "43.41% done\n",
      "Lyrics not found for Invasion Of The Flat Booty B*****s - Too $hort\n",
      "43.68% done\n",
      "43.96% done\n",
      "44.24% done\n",
      "44.52% done\n",
      "44.8% done\n",
      "45.08% done\n",
      "45.35% done\n",
      "45.63% done\n",
      "45.91% done\n",
      "Lyrics not found for Come Close To Me - Common Featuring Mary J. Blige\n",
      "46.19% done\n",
      "46.47% done\n",
      "Lyrics not found for Bartender Song - Rehab\n",
      "46.74% done\n",
      "47.02% done\n",
      "Lyrics not found for Shake That Sh** - Shawnna Featuring Ludacris\n",
      "Lyrics not found for 24's - T.I.\n",
      "47.3% done\n",
      "47.58% done\n",
      "47.86% done\n",
      "Lyrics not found for The Best Man I Can Be - Ginuwine, R.L., Tyrese, Case\n",
      "48.14% done\n",
      "48.41% done\n",
      "48.69% done\n",
      "48.97% done\n",
      "Lyrics not found for Ain't No Playa - Rappin' 4-Tay\n",
      "49.25% done\n",
      "49.53% done\n",
      "49.81% done\n",
      "50.08% done\n",
      "50.36% done\n",
      "50.64% done\n",
      "50.92% done\n",
      "51.2% done\n",
      "Lyrics not found for A Broken Wing - Jordin Sparks\n",
      "51.47% done\n",
      "51.75% done\n",
      "52.03% done\n",
      "52.31% done\n",
      "52.59% done\n",
      "52.87% done\n",
      "53.14% done\n",
      "Lyrics not found for How Do U Want It/California Love - 2Pac Featuring K-Ci And JoJo\n",
      "53.42% done\n",
      "53.7% done\n",
      "53.98% done\n",
      "54.26% done\n",
      "54.54% done\n",
      "54.81% done\n",
      "55.09% done\n",
      "55.37% done\n",
      "55.65% done\n",
      "55.93% done\n",
      "56.2% done\n",
      "56.48% done\n",
      "56.76% done\n",
      "57.04% done\n",
      "57.32% done\n",
      "57.6% done\n",
      "57.87% done\n",
      "58.15% done\n",
      "58.43% done\n",
      "Lyrics not found for Fly S**t Only - Future\n",
      "58.71% done\n",
      "Lyrics not found for Light Your A** On Fire - Busta Rhymes\n",
      "58.99% done\n",
      "Lyrics not found for I Don't F**k With You - Big Sean Featuring E-40\n",
      "59.27% done\n",
      "59.54% done\n",
      "59.82% done\n",
      "Lyrics not found for I'm In Miami Trick - LMFAO\n",
      "60.1% done\n",
      "60.38% done\n",
      "60.66% done\n",
      "Lyrics not found for F**k Today - Lil Wayne Featuring Gudda\n",
      "60.93% done\n",
      "61.21% done\n",
      "Lyrics not found for New Jack Hustler (Nino's Theme) (From \"New Jack City\") - Ice-T\n",
      "Lyrics not found for Jeeps, Lex Coups, Bimaz & Benz - Lost Boyz\n",
      "Lyrics not found for Independent Women Part I - Destiny's Child\n",
      "61.49% done\n",
      "61.77% done\n",
      "Lyrics not found for I Who Have Nothing - Jordin Sparks\n",
      "62.05% done\n",
      "62.33% done\n",
      "Lyrics not found for Knock-N-Boots - Wreckx-N-Effect\n",
      "62.6% done\n",
      "62.88% done\n",
      "63.16% done\n",
      "Lyrics not found for Hot Boy - Bobby Shmurda\n",
      "63.44% done\n",
      "63.72% done\n",
      "Lyrics not found for Down A** Chick - Ja Rule Featuring Charli \"Chuck\" Baltimore\n",
      "64.0% done\n",
      "Lyrics not found for Feelin' Myself - will.i.am Featuring Miley Cyrus, French Montana, Wiz Khalifa & DJ Mustard\n",
      "Lyrics not found for Holding My Heart - Bang\n",
      "64.27% done\n",
      "Lyrics not found for Drums - Jon & Robin\n",
      "64.55% done\n",
      "64.83% done\n",
      "Lyrics not found for Cupid's Chokehold/Breakfast In America - Gym Class Heroes Featuring Patrick Stump\n",
      "65.11% done\n",
      "65.39% done\n",
      "Lyrics not found for Roll Out (My Business) - Ludacris\n",
      "65.66% done\n",
      "Lyrics not found for Back That Thang Up - Juvenile Featuring Mannie Fresh & Lil' Wayne\n",
      "65.94% done\n",
      "66.22% done\n",
      "66.5% done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.78% done\n",
      "67.06% done\n",
      "67.33% done\n",
      "Lyrics not found for I Am The Champion - B.o.B\n",
      "67.61% done\n",
      "67.89% done\n",
      "68.17% done\n",
      "68.45% done\n",
      "68.73% done\n",
      "69.0% done\n",
      "Lyrics not found for Now & Later - Sage The Gemini\n",
      "69.28% done\n",
      "Lyrics not found for My Choppa Hate N****s - 21 Savage & Metro Boomin\n",
      "69.56% done\n",
      "69.84% done\n",
      "Lyrics not found for Ice Tray - Quavo & Lil Yachty\n",
      "70.12% done\n",
      "70.4% done\n",
      "70.67% done\n",
      "70.95% done\n",
      "Lyrics not found for Start This S**t Off Right - Lil Wayne Featuring Ashanti & Mack Maine\n",
      "71.23% done\n",
      "Lyrics not found for Request Line - The Black Eyed Peas Featuring Macy Gray\n",
      "71.51% done\n",
      "71.79% done\n",
      "72.06% done\n",
      "Lyrics not found for B-Please - Snoop Dogg Featuring Xzibit & Nate Dogg\n",
      "72.34% done\n",
      "72.62% done\n",
      "Lyrics not found for All Through The Night - Tone-Loc\n",
      "72.9% done\n",
      "73.18% done\n",
      "73.46% done\n",
      "73.73% done\n",
      "Lyrics not found for Real Wild Child - Ivan\n",
      "Lyrics not found for The 81 - Candy & The Kisses\n",
      "74.01% done\n",
      "74.29% done\n",
      "Lyrics not found for Ryde Or Die, Chick - The Lox Featuring Timbaland And EVE\n",
      "74.57% done\n",
      "Lyrics not found for Bubble Gum Music - Rock & Roll Double Bubble\n",
      "74.85% done\n",
      "75.13% done\n",
      "Lyrics not found for #1 Dee Jay - Goody Goody\n",
      "Lyrics not found for 1 Night - Lil Yachty\n",
      "75.4% done\n",
      "75.68% done\n",
      "75.96% done\n",
      "Lyrics not found for P**** Print - Gucci Mane Featuring Kanye West\n",
      "Lyrics not found for Back Door Man - Derek\n",
      "Lyrics not found for Same Ol' Shit - MC Ren\n",
      "76.24% done\n",
      "76.52% done\n",
      "76.79% done\n",
      "77.07% done\n",
      "77.35% done\n",
      "77.63% done\n",
      "77.91% done\n",
      "Lyrics not found for Doin' The Continental Walk - Danny & The Juniors\n",
      "Lyrics not found for Runnin' - 2Pac, Notorious B.I.G., Radio, Dramacydal & Stretch\n",
      "78.19% done\n",
      "78.46% done\n",
      "Lyrics not found for Roberta - Bones\n",
      "78.74% done\n",
      "Lyrics not found for Definition - Mos Def & Kweli Are Black Star\n",
      "79.02% done\n",
      "79.3% done\n",
      "Lyrics not found for That's How It Is (It's Like That) - Redman Featuring K-Solo\n",
      "Lyrics not found for Cop That Sh#! - Timbaland & Magoo Featuring Missy Elliott\n",
      "79.58% done\n",
      "Lyrics not found for Still Will - 50 Cent Featuring Akon\n",
      "79.86% done\n",
      "80.13% done\n",
      "Lyrics not found for Cop Shot The Kid - Nas Featuring Kanye West\n",
      "80.41% done\n",
      "80.69% done\n",
      "Lyrics not found for Bottom Of The Bottle - Curren$Y Featuring August Alsina & Lil Wayne\n",
      "80.97% done\n",
      "Lyrics not found for Same Yung N***a - Gunna Featuring Playboi Carti\n",
      "Lyrics not found for Step To This - Master P Featuring D.I.G.\n",
      "81.25% done\n",
      "81.52% done\n",
      "Lyrics not found for How Can You Mistreat The One You Love - Jean & The Darlings\n",
      "81.8% done\n",
      "82.08% done\n",
      "Lyrics not found for F**k That Check Up - Meek Mill Featuring Lil Uzi Vert\n",
      "82.36% done\n",
      "Lyrics not found for Lean & Dabb - iLoveMemphis\n",
      "82.64% done\n",
      "82.92% done\n",
      "83.19% done\n",
      "83.47% done\n",
      "Lyrics not found for One - Usher & Michelle Chamuel\n",
      "83.75% done\n",
      "Lyrics not found for Stop Snitching - YG\n",
      "Lyrics not found for Oo-La-La-Limbo - Danny & The Juniors\n",
      "84.03% done\n",
      "84.31% done\n",
      "84.59% done\n",
      "Lyrics not found for Soul Sauce (Guacha Guaro) - Cal Tjader\n",
      "84.86% done\n",
      "85.14% done\n",
      "85.42% done\n",
      "Lyrics not found for Too Hotty - Quavo, Takeoff & Offset\n",
      "85.7% done\n",
      "Lyrics not found for Touch Me When We're Dancing - Bama\n",
      "85.98% done\n",
      "86.25% done\n",
      "Lyrics not found for Trophies - Young Money Featuring Drake\n",
      "86.53% done\n",
      "86.81% done\n",
      "87.09% done\n",
      "Lyrics not found for Twistin' All Night Long - Danny & The Juniors with Freddy Cannon\n",
      "87.37% done\n",
      "87.65% done\n",
      "87.92% done\n",
      "88.2% done\n",
      "Lyrics not found for Untitled 02 l 06.23.2014. - Kendrick Lamar\n",
      "Lyrics not found for Untitled 07 l Levitate - Kendrick Lamar\n",
      "88.48% done\n",
      "Lyrics not found for Use It Up And Wear It Out - Pat & Mick\n",
      "88.76% done\n",
      "Lyrics not found for Very Special - Big Daddy Kane Feat. Spinderella, L. Williams & K. Anderson\n",
      "89.04% done\n",
      "Lyrics not found for W O R K I N M E - Quavo\n",
      "89.32% done\n",
      "Lyrics not found for The Joint - EPMD\n",
      "89.59% done\n",
      "Lyrics not found for The Loop - Johnny Lytle\n",
      "89.87% done\n",
      "90.15% done\n",
      "Lyrics not found for When You're Mad - Ne-Yo\n",
      "90.43% done\n",
      "90.71% done\n",
      "Lyrics not found for Who Got The Props - Black Moon\n",
      "Lyrics not found for Who The F*** Is That? - Dolla Featuring T-Pain & Tay Dizm\n",
      "90.98% done\n",
      "Lyrics not found for Why I Love You So Much/Ain't Nobody - Monica\n",
      "91.26% done\n",
      "Lyrics not found for Wild Thing - Tone-Loc\n",
      "91.54% done\n",
      "91.82% done\n",
      "92.1% done\n",
      "Lyrics not found for Woo-Hah!! Got You All In Check/Everything Remains Raw - Busta Rhymes\n",
      "92.38% done\n",
      "92.65% done\n",
      "92.93% done\n",
      "93.21% done\n",
      "93.49% done\n",
      "93.77% done\n",
      "94.05% done\n",
      "Lyrics not found for Way Too Cold - Kanye West Featuring DJ Khaled\n",
      "Lyrics not found for We Ain't Goin' Out Like That - Cypress Hill\n",
      "Lyrics not found for We Are One (Ole Ola) [The 2014 FIFA World Cup Official Song] - Pitbull Featuring Jennifer Lopez & Claudia Leitte\n",
      "94.32% done\n",
      "94.6% done\n",
      "94.88% done\n",
      "Lyrics not found for Welcome Back - Mase\n",
      "95.16% done\n",
      "Lyrics not found for What Happened To That Boy - Baby Featuring Clipse\n",
      "95.44% done\n",
      "95.72% done\n",
      "Lyrics not found for What You Got - Duke & The Drivers\n",
      "Lyrics not found for What You Want - Mase Featuring Total\n",
      "95.99% done\n",
      "96.27% done\n",
      "96.55% done\n",
      "Lyrics not found for The Swalk - Notorious\n",
      "96.83% done\n",
      "97.11% done\n",
      "97.38% done\n",
      "Lyrics not found for Things'll Never Change/Rapper's Ball - E-40 Featuring Bo-Rock\n",
      "97.66% done\n",
      "97.94% done\n",
      "98.22% done\n",
      "98.5% done\n",
      "98.78% done\n",
      "Lyrics not found for Tonight Is The Night - Outasight\n",
      "99.05% done\n",
      "99.33% done\n",
      "99.61% done\n",
      "99.89% done\n"
     ]
    }
   ],
   "source": [
    "lyrics_list = []\n",
    "\n",
    "for i, row in df_final.iterrows():\n",
    "    if (i % 10 == 0): \n",
    "        print (str(round(i/len(df_final) * 100, 2)) + '% done')\n",
    "    artist = row['Performer']\n",
    "    song = row['Song']\n",
    "    lyrics = get_lyrics(song, artist)\n",
    "    # print(lyrics)\n",
    "    lyrics_list.append(lyrics)\n",
    "    \n",
    "df_final['Lyrics'] = lyrics_list\n",
    "df_final.to_csv(\"../data/Hot100DataWithNanLyrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with no lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with no lyrics\n",
    "df_final.dropna(subset=['Lyrics'], inplace=True)\n",
    "\n",
    "# export to csv\n",
    "df_final.to_csv(\"../data/Hot100Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dropped 150 songs from our dataset because we were unable to find the lyrics for them using the Genius API. After dropping these songs, we now have a total of **3448 observations** in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range of Years for Dataset\n",
    "\n",
    "Check to see if the minimum and maximum years are still 1958 - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959 2019\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(min(df_final['Year']), max(df_final['Year']))\n",
    "\n",
    "# Find how many songs are in the earliest year\n",
    "print(len([df_final['Year'] == '1959']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our earliest year is 1959 and we only have one song from that year, we can still group it with the 1960s. We've decided to keep this rather than remove it just because our analysis still covers a span of approximately 60 years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin our analysis, we need to check if there are any unexpected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark><strong>DUPLICATE SONGS IN DATAFRAME</strong></mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3296 3448\n"
     ]
    }
   ],
   "source": [
    "# check to see if no songs were randomly lost\n",
    "# rap songs in our original data set\n",
    "og_rap_count = len([song for song in df_rap['spotify_genre'] if 'rap' in song])\n",
    "# total rap songs after clean-up (must account for 150 songs manually removed because we were unable to find their lyrics)\n",
    "now_rap_count = df_final.shape[0]\n",
    "\n",
    "# check to see if each song name is unique and that we have a total of 3448 unique songs\n",
    "unique_song_count = len(df_final['Song'].unique())\n",
    "\n",
    "\n",
    "\n",
    "# assertion checks\n",
    "assert og_rap_count == now_rap_count + 150 # manually removed songs\n",
    "print(unique_song_count, now_rap_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Results\n",
    "\n",
    "### Notes\n",
    "- TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "\n",
    "- Spacy\n",
    "- TF-IDF\n",
    "\n",
    "We use Spacy to initially remove any stop words from the dataset in order to get a finalized dataframe that will contain the most common words across decades. We remove stop words because we want to able to 1) convert contractions into regular words (such as slang) and 2) correct the spelling of words (because some words may be weirdly mispelled) so that the data can actually be used for analysis and data visualizations.\n",
    "\n",
    "We use TF-IDF on the original rap lyrics (`df_final`) so that each word can be considered _within its context_. We don't want to abstract each word from its surrounding and then calculate TF-IDF, because then we lose the exact number of songs in our collection of songs that contain each word, which is a vital part of the inverse document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a dictionary for contractions in English retrived from the web. This is not included in the language toolkit, thus it is manuallly added here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {     \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I had\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"iit will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here the pipeline of cleaning the data for text analysis is prepared, such as removing punctuation, stopwords, digits, correcting spelling, and everything would be in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "punct=punctuation+'â'+'â'+'â'+'â'\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    \"\"\"\n",
    "    expand english contractions\n",
    "    \"\"\"\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "            if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def autospell(text):\n",
    "    \"\"\"\n",
    "    correct the spelling of the word.\n",
    "    \"\"\"\n",
    "    spell = Speller(lang='en',fast=True)\n",
    "    spells = [spell(w) for w in (nltk.word_tokenize(text))]\n",
    "    return \" \".join(spells)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner']) #load small english core library\n",
    "stops = stopwords.words(\"english\") #load stopwords\n",
    "\n",
    "\n",
    "\n",
    "def normalize(text, lowercase, remove_stopwords):\n",
    "    '''\n",
    "    clean the text into desired format for text analysis\n",
    "    '''\n",
    "    text = expand_contractions(text,contractions_dict) #expand english contractions\n",
    "    text = autospell(text) #correct spelling\n",
    "    #text = ' '.join([w.lower() for w in nltk.word_tokenize(text)])  #lowercase\n",
    "    text = re.sub('<[^<]+?>','', text)  #remove brackets\n",
    "    text = ''.join(c for c in text if not c.isdigit())  #remove numbers\n",
    "    text = ''.join(c for c in text if c not in punct)  #remove punctuations\n",
    "    if lowercase:\n",
    "        text = text.lower()  #lowercase\n",
    "    text = nlp(text)\n",
    "    lemmatized = list()\n",
    "    for word in text:\n",
    "        lemma = word.lemma_.strip() #tokenize\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):  #remove stopwords\n",
    "                lemmatized.append(lemma)\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the dataframe is filtered to contain only English lyrics, and the cleaning function is applied to the lyric column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark><strong>BOTTOM CELL NEEDS TO BE FIXED</strong></mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/Hot100Data.csv')\n",
    "df['lang']=df.Lyrics.apply(detect)\n",
    "df=df[df['lang']=='en']\n",
    "df['After_Clean'] = df['Lyrics'].apply(normalize, lowercase=True, remove_stopwords=True)\n",
    "df.drop(columns=['Unnamed: 0','lang'], inplace=True)\n",
    "df.to_csv('cleandata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for calculating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(\"../data/cleandata.csv\",converters={'After_Clean': eval})\n",
    "df_final['doc']=df_final['After_Clean'].apply(lambda x:' '.join(x))\n",
    "\n",
    "# term frequency\n",
    "# t: term\n",
    "# d: document\n",
    "# returns the number of times t appears in d\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "# inverse document frequency\n",
    "# t: term\n",
    "# N: total number of documents\n",
    "# returns the IDF of a term\n",
    "def idf(t, N, docCount):\n",
    "    n_t = abs(docCount[t]) # number of documents containing t\n",
    "    return math.log(N/n_t, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a decade dataframe df and returns another dataframe containing the TF-IDF values for each word in the lyrics for df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_df(df):\n",
    "    # dictionary that keeps track of words and the number of documents they appear in\n",
    "    docCount = defaultdict(int)\n",
    "\n",
    "    # build dictionary\n",
    "    for i, row in df.iterrows():\n",
    "        words = row['After_Clean']\n",
    "        r = row['doc']\n",
    "        seen = set() # words seen in current document\n",
    "        for w in words:\n",
    "            # increment docCount for word w if it is in current document\n",
    "            if w not in seen:\n",
    "                docCount[w] += 1\n",
    "                seen.add(w)\n",
    "\n",
    "    # dictionary that keeps track of words and their tfidf values\n",
    "    tfidf_dict = defaultdict(int)\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        words = row['After_Clean']\n",
    "        r = row['doc']\n",
    "        for w in words:\n",
    "            my_tf = tf(w, r)\n",
    "            my_idf = idf(w, len(df), docCount)\n",
    "            tfidf_dict[w] += my_tf * my_idf\n",
    "    \n",
    "    # sort by tfidf values, descending\n",
    "    tfidf_counts = [(tfidf_dict[w], w) for w in tfidf_dict]\n",
    "    tfidf_counts.sort()\n",
    "    tfidf_counts.reverse()\n",
    "    \n",
    "    # turn into dataframe\n",
    "    tfidf_df = pd.DataFrame(tfidf_counts)\n",
    "    tfidf_df.columns = [\"TF-IDF\", \"Term\"]\n",
    "    \n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate df_final by decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5960 = df_final[df_final['Year'] <= 1969] # includes 1959, 35 observations\n",
    "df_70 = df_final.loc[(df_final['Year'] >= 1970) & (df_final['Year'] <= 1979)] # 14 observations\n",
    "df_80 = df_final.loc[(df_final['Year'] >= 1980) & (df_final['Year'] <= 1989)] # 53 observations\n",
    "df_90 = df_final.loc[(df_final['Year'] >= 1990) & (df_final['Year'] <= 1999)] # 483 observations\n",
    "df_00 = df_final.loc[(df_final['Year'] >= 2000) & (df_final['Year'] <= 2009)] # 1050 observations\n",
    "df_10 = df_final.loc[(df_final['Year'] >= 2010) & (df_final['Year'] <= 2019)] # 1813 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all TF-IDF dataframes for each decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_5960 = get_tfidf_df(df_5960)\n",
    "tfidf_70 = get_tfidf_df(df_70)\n",
    "tfidf_80 = get_tfidf_df(df_80)\n",
    "tfidf_90 = get_tfidf_df(df_90)\n",
    "tfidf_00 = get_tfidf_df(df_00)\n",
    "tfidf_10 = get_tfidf_df(df_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the top 10 words with the highest TF-IDF values in each decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        TF-IDF   Term\n",
      "0  3528.527425  giddy\n",
      "1  1365.798303     la\n",
      "2   675.382202    mia\n",
      "3   675.382202  hound\n",
      "4   675.382202    dog\n",
      "5   675.382202   cara\n",
      "6   662.797982    yes\n",
      "7   652.137928  loose\n",
      "8   571.461853    cry\n",
      "9   546.360327   baby\n",
      "        TF-IDF    Term\n",
      "0  1821.569301     get\n",
      "1   755.517648      ta\n",
      "2   428.651885    numb\n",
      "3   418.798245     hot\n",
      "4   313.148244    want\n",
      "5   224.641095  summer\n",
      "6   197.752941    work\n",
      "7   166.484314   night\n",
      "8   165.042437      la\n",
      "9   165.042437    fade\n",
      "        TF-IDF    Term\n",
      "0  9339.339425    wild\n",
      "1  9339.339425     bow\n",
      "2  3763.164251   break\n",
      "3  3098.438040      ah\n",
      "4  3027.243974       g\n",
      "5  2965.754496  tricky\n",
      "6  2439.434427    jump\n",
      "7  2371.934420    push\n",
      "8  2219.143044    mary\n",
      "9  2056.590288    type\n",
      "         TF-IDF    Term\n",
      "0  22980.031111     dat\n",
      "1  17964.259176       e\n",
      "2  13438.207902    jump\n",
      "3  12980.375201    wake\n",
      "4  11759.790352  weasel\n",
      "5  11037.136383       g\n",
      "6  11016.828822     duh\n",
      "7  10944.511294      ha\n",
      "8   9065.855760     yes\n",
      "9   8084.393376    king\n",
      "          TF-IDF   Term\n",
      "0  129516.286769     la\n",
      "1   32887.481644     da\n",
      "2   25645.327156   boom\n",
      "3   23153.932591    bay\n",
      "4   22696.790845  whoop\n",
      "5   21097.994045     oh\n",
      "6   18524.187733    ooh\n",
      "7   16772.051986     ba\n",
      "8   15867.327993   book\n",
      "9   15638.662945   love\n",
      "         TF-IDF     Term\n",
      "0  78670.614744      wop\n",
      "1  65038.171463       la\n",
      "2  60183.566331       oh\n",
      "3  59070.361629       lu\n",
      "4  48900.801046      low\n",
      "5  48518.580936  versace\n",
      "6  40825.051079   howbow\n",
      "7  37961.048771  diddily\n",
      "8  34845.596914      dah\n",
      "9  31634.612998      wit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tfidf_5960[:10])\n",
    "print(tfidf_70[:10])\n",
    "print(tfidf_80[:10])\n",
    "print(tfidf_90[:10])\n",
    "print(tfidf_00[:10])\n",
    "print(tfidf_10[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram\n",
    "\n",
    "First we will include a histogram of the total raw word counts by decade. Here, we can see the distribution of word counts per decade to see if there are any outstanding ones.\n",
    "\n",
    "Visualization Description:\n",
    "- Bins will be decades\n",
    "- X-axis: number of words in a song\n",
    "- Y-axis: number of songs with that word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram code for total raw word count here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark><strong>INSERT ANALYSIS ANSWERS HERE</strong></mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Graphs\n",
    "\n",
    "Next, from the Spacy's dataset, we will construct multiple bar graphs and place them side by side to visually see the most common words of each decade. This way, we can start to get an idea of the differences between each decade of rap music.\n",
    "\n",
    "Visualization Description:\n",
    "- Data: Spacy's\n",
    "- Each graph will have top 15 words\n",
    "- Top 10 words in a bar graph per decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5e9b823092f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop15\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"counts\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Blues_d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top 10 Words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_final' is not defined"
     ]
    }
   ],
   "source": [
    "top15 = df_final.iloc[0:15]\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\"words\", \"counts\", data=top15, palette=\"Blues_d\").set_title(\"Top 10 Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, from the TF-IDF dataset, we will again construct mutliple bar graphs and place them side by side to visually see the set of unique words from each decade. This way, we will clearly get to see if there is a vast difference of overarching themes from one decade to another.\n",
    "\n",
    "Visualization Description:\n",
    "- Data: TF-IDF's\n",
    "- Top 15 unique words in a bar graph per decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Graph code for most unique words per decade here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark><strong>INSERT ANALYSIS ANSWERS HERE</strong></mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark><strong>LEXICAL DENSITY? MAYBE?</strong></mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Graphs\n",
    "Lastly, we will look into line graphs so we can get an idea of how the most common and unique words have changed over time.\n",
    "\n",
    "The bottom line graph visualizes the most common words.\n",
    "\n",
    "Visualization Description:\n",
    "- Data: Spacy's\n",
    "- X-axis: time\n",
    "- Y-axis: word count during that time\n",
    "- Each line would be most common regular words shared amongst decades\n",
    "- Have at minimum 10-20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Graph code for most common words per decade here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark><strong>INSERT ANALYSIS ANSWERS HERE</strong></mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the bottom line graph visualizes the unique words.\n",
    "\n",
    "Visualization Description:\n",
    "- Data: TF-IDF's\n",
    "- X-axis: time\n",
    "- Y-axis: word count during that time\n",
    "- Each line would be most popular topics shared amongst decades\n",
    "- Have at minimum 10-20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Graph code for unique words per decade here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark><strong>INSERT ANALYSIS ANSWERS HERE</strong></mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What approaches did you use? Why?\n",
    "    - How did you go about cleaning the data and looking at lyrics? Why did you think this was the best way to go about conducting this analysis/study?\n",
    "2. What were the results?\n",
    "    - Verbatim: talk about the data and what you found. State any crazy or interesting findings here.\n",
    "3. What was your interpretation of these findings?\n",
    "    - How would you interpret this data? Connect this back to your question and hypothesis? Has rap music changed according to the data? Or has it remained the same in terms of topics?\n",
    "4. TODO. Add more questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark><strong>BOTTOM CELL NEEDS TO BE FIXED</strong></mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics and Privacy\n",
    "\n",
    "We will be looking at songs that have been the most popular in each decade. This will not introduce any bias in our dataset aside from the fact that these songs were originally placed due to the OPINIONS of the listeners. \n",
    "\n",
    "Our dataset will be formed by looking at the lyrics for each song that are outputted by the lyrics extractor package, but we will have to check for credibility by cross-verifying. Because lyrics extractor offers various mediums for accessing song lyrics, we can easily compare lyrics by using a set of relational operators on their string representations. \n",
    "\n",
    "Again, our selection of songs will not have any bias aside from the fact that Wikipedia chooses their cumulative lists based on popularity. Therefore, it may leave out rap songs that are not as popular and may not be a true representation of ALL rap songs for each decade. In our project, we will explicitly state that we will only be performing an analysis on the most popular songs for each decade. \n",
    "\n",
    "From the explanation above, because we are not looking at ALL rap music (which is extremely difficult because lots of rap music are not even publicized), it may discredit our work if we do not carefully state all the parameters in our study (ie what we've included, excluded, and took into or out of consideration).\n",
    "\n",
    "In terms of data privacy, every lyric we find is for public-use, as lyrics extractor will only pull from public databases. Therefore, we will not break any TOS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark><strong>BOTTOM CELL NEEDS TO BE FIXED</strong></mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion\n",
    "\n",
    "- TODO"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
