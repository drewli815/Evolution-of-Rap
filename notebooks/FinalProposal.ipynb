{
 "cells": [
  {
   "source": [
    "# Overview"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Names and Githubs\n",
    "\n",
    "- Andrew Li - drewli815\n",
    "- Jialin Shan - j5shan\n",
    "- Sameer Ahmed - the3L3M3NT\n",
    "- Lacey Umamoto - lumamoto\n",
    "- Rickesh Khilnani - rick10101221\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Research Question\n",
    "\n",
    "In the past 60 years, how has rap music's lyrics and overarching themes evolved into the repetitive, monotonous genre it has become today? Can we form the conclusion that the main topics in rap have not changed after performing an analysis of the Billboard's number-one rap singles for each decade ranging from the 1960s to the 2010s?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Background Research\n",
    "\n",
    "- We realized that contemporary music is repetitive because a lot of songs have a repeating theme in the form of either lyrics or the type of beat. This is very evident in contemporary rap. A lot of rap songs cover the same topics: guns, liquor, drugs, money, sex, getting famous, being rich, etc. But one has to consider the type of beat that rap music uses. It's quite hard to tell the difference from one rap song to another if you were to just listen to the instrumental versions. The dull and repetitive nature of rap comes from exactly this.\n",
    "\n",
    "- Therefore, in order to measure the monotony, we will be accumulating data from thousands of songs ranging from the 1960s - 2010s and performing an analysis on their overarching themes by using TF-IDF on the lyrics of all 3800 rap songs and looking at the differences in unique and common words. The American rap genre will be considered to have evolved from innovative to monotonous if rap music from the 1960s covers a range of topics other than the ones listed above as opposed to contemporary rap. In the 20th century, rap music was much more impactful and many pieces pushed to make public change. Nowadays, that isn't always the case. At least, not as often.\n",
    "\n",
    "- The overarching theme of each song will be the most repeated keyword (after removing common english words), word counts will be some sort of map or Counter that associates frequencies with words. We will primarily be using Genius' API to gather lyrics.\n",
    "\n",
    "- We believe there should be a change in the music industry. Today's music industry makes music that is solely for 'instant gratification' and is completely shallow. (Refer to the second resource in the *Past Studies* section below)\n",
    "\n",
    "- We all listen to and enjoy music.\n",
    "\n",
    "- Past Studies\n",
    "    - There is an [article](https://www.rapanalysis.com/2015/08/the-23-most-repetitive-rappers/) that has illustrated the 23 most repetitive rappers, the highest being Will.I.Am and Kid Cudi. The analysis looked at how often certain keywords were said, but only compared it from artist to artist.\n",
    "    - In this [tiny piece](https://roundup.brophyprep.org/index.php/2012/03/popular-hip-hop-music-praises-shallow-superficial-decadence/) (written 9 years ago), talks about how music used to promote good values, but in the 21st century, it has completely shifted to a shallow and superficial genre. He adds a quote from one of Drake's songs that only focuses on the aspects of life that are instantly gratifying: getting money, drinking, and smoking, namely.\n",
    "\n",
    "- References (include links):\n",
    "    - [Billboard's Hot Weekly Charts](https://data.world/kcmillersean/billboard-hot-100-1958-2017) \n",
    "    - [Lyrics-Extractor Package for Python](https://pypi.org/project/lyrics-extractor/)\n",
    "        - This can be used for retrieving and cross-referencing lyrics for data validity. Uses Genius' API\n",
    "    - [Language Detector](https://pypi.org/project/langdetect/) \n",
    "    - [Spotify's Web API](https://spotipy.readthedocs.io/en/2.16.0/) (canceled)\n",
    "    - [Spotify](https://www.spotify.com/us/) (canceled)\n",
    "    - [MusixMatch API for lyrics](https://developer.musixmatch.com/) (canceled)  \n",
    "    - Google Sheets to store accumulated data for each decade (at the end of project for reference)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Hypothesis\n",
    "\n",
    "After conducting a thorough analysis of music repetition, we believe that there exists a trend in contemporary rap music. This trend will be evident after comparing the overarching themes of rap music for each decade. We expect that, through these observations, we will find a repetitive trend because contemporary rap music focuses on the same topics for each song whereas older rap music used to cover a diverse range of topics so that each song and artist was refreshing and unique."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data\n",
    "\n",
    "- What variables: song name, song title, song year, lyrics for song (dataframe 1), and keywords with their respective counts (dataframe 2). Dataframe 1 will contain the data for all 3800 songs and dataframe 2 will divide the respective counts for each decade in separate columns.\n",
    "\n",
    "- How many observations: Varies depending on how many songs are on each Billboard for each decade. We will have to find a way to normalize our data so that the number of songs for each decade are the same. In total, however, we will trim a dataset of 28000 songs down to 3800 which are specifically rap songs.\n",
    "\n",
    "- Who/what/how would these data be collected: \n",
    "    - Who\n",
    "        - Each week, teams will be formed and tasks will be assigned to each subteam so that we can achieve maximum efficiency in data collection and analysis. Everyone's time would not be spent completing the same task and we will finish the project quicker. For example, team A may be tasked with data collection while team B will be in charge of parsing through it for cleaning (removing stop words, stemming, etc.)\n",
    "    - What\n",
    "        - At first, we will be collecting song names, artists, and the date at which each song reached number one. Then, using lyrics extractor, we will find the lyrics for each song. At this point, this is all the data collection we need. Additional steps include parsing through the data with TF-IDF, removing common English words, and developing an extra dataframe for holding word counts for each decade..\n",
    "    - How\n",
    "        - First using Billboard's number-one songs and exporting as a csv file. Then, we will read it in as a dataframe so that we can then iterate through each song title and run the lyrics extractor package on it. Then we will simply store each lyric body in the dataframe, remove any common English words from each lyric body, and iterate through all of the lyrics and associate counts with each word.\n",
    "\n",
    "- How would these data be stored/organized: Pandas dataframe which will be used for building visualizations. Again, the data will be organized as two separate data frames for processing (see above)\n",
    "\n",
    "- What kind of songs are you collecting: We will be collecting Billboard's number-one songs for each decade from the 1960s to the 2010s. The number of songs for each decade varies, so we will have to establish a numerical bottleneck.\n",
    "\n",
    "- Dataset 1:\n",
    "    - Dataset Name: Billboard Hot Weekly Charts\n",
    "    - [Link to the dataset](https://data.world/kcmillersean/billboard-hot-100-1958-2017)\n",
    "    - Number of total observations: 28500\n",
    "    - Truncated observation count for specifically 'rap': 3850\n",
    "\n",
    "- Notes\n",
    "    - We will ONLY be looking at rap songs for this analysis. This is purely from Billboard's Hot Weekly charts so that we get a rough estimate of what the most popular songs are for each decade. Therefore, this may not capture ALL topics in rap over the last 6 decades.\n",
    "    - Because we have a method of retrieving all of the lyrics for each song in our data set, we know that we will have more than 25000+ words and 3800+ songs to parse through, which is more than enough to model the overarching themes for each decade for comparison with one another.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from autocorrect import Speller\n",
    "from collections import Counter \n",
    "import spacy\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import lyricsgenius\n",
    "from langdetect import detect"
   ]
  },
  {
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Cut down 28000 observations to 3800 specifically pertaining to rap, merging columns to get a single ID, dropping identical songs, and creating a final dataframe to hold the lyrics of every song."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius = lyricsgenius.Genius(\"-VH73uIIYTXillAxkhh-uuEUfTNPXLVT25-EcyNlfBnXK2kSPidRXPQCsDumgKmw\")\n",
    "\n",
    "df = pd.DataFrame(pd.read_excel(\"./data/Hot 100 Audio Features.xlsx\"))\n",
    "df_hotstuff = pd.DataFrame(pd.read_csv(\"./data/Hot Stuff.csv\"))"
   ]
  },
  {
   "source": [
    "### Cutting Down Data\n",
    "\n",
    "Here, we are dropping any obserations without any genre specified, assuming that they were not considered rap songs because they weren't explicitly tagged with it. This **ALSO** excludes songs without a rap tag entirely, including 'Hip Hop' songs (hip hop songs have an explcit 'hip hop' tag). Therefore, we are solely focusing on rap music in the entire data set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop songs without genres\n",
    "df.dropna(subset=['spotify_genre'], inplace=True)\n",
    "\n",
    "# get songs with rap genre\n",
    "df_rap = pd.DataFrame()\n",
    "for index, row in df.iterrows():\n",
    "    genres = df.spotify_genre.squeeze()[index]\n",
    "    if 'rap' in genres:\n",
    "        df_rap = df_rap.append(row)\n",
    "\n",
    "# drop duplicate songs (songs with same songID)\n",
    "df_rap = df_rap.drop_duplicates(subset=['SongID'], keep='first')"
   ]
  },
  {
   "source": [
    "### Clean Up Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df_rap and df_hotstuff to get weekID\n",
    "df_merge = pd.merge(df_rap, df_hotstuff, how='left')\n",
    "\n",
    "# drop songs with no weekID\n",
    "df_merge.dropna(subset=['WeekID'], inplace=True)\n",
    "\n",
    "# drop duplicate songs (songs with same songID)\n",
    "df_merge = df_merge.drop_duplicates(subset=['SongID'],keep='first')"
   ]
  },
  {
   "source": [
    "### Check for unexpected results\n",
    "\n",
    "- TODO: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Getting Lyrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lyricsgenius'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a7eefb0f48dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlyricsgenius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgenius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlyricsgenius\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenius\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-VH73uIIYTXillAxkhh-uuEUfTNPXLVT25-EcyNlfBnXK2kSPidRXPQCsDumgKmw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lyricsgenius'"
     ]
    }
   ],
   "source": [
    "# get years\n",
    "years = []\n",
    "for index, row in df_merge.iterrows():\n",
    "    weekID = df_merge.WeekID[index]\n",
    "    year = datetime.strptime(weekID, \"%m/%d/%Y\").year\n",
    "    years.append(year)\n",
    "df_merge['Year'] = years\n",
    "\n",
    "# get final dataframe with year, performer, and song\n",
    "df_final = df_merge[['Year', 'Performer', 'Song']]\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "# get lyrics\n",
    "glyrics = []\n",
    "for index, row in df_final.iterrows():\n",
    "    try:\n",
    "        s = genius.search_song(row['Song'], row['Performer'])\n",
    "        lyric = s.lyrics\n",
    "    except:\n",
    "        lyric = np.NaN\n",
    "    glyrics.append(lyric)\n",
    "df_final['Lyrics'] = glyrics\n",
    "\n",
    "# drop rows with no lyrics\n",
    "df_final.dropna(subset=['Lyrics'], inplace=True)"
   ]
  },
  {
   "source": [
    "### Range of Years for Dataset\n",
    " - TODO:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Data Analysis and Results\n",
    "\n",
    "### Notes\n",
    "- TODO: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Algorithms\n",
    "\n",
    "- Spacy\n",
    "- TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {     \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I had\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"iit will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "punct=punctuation+'’'+'“'+'”'+'–'\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    \"\"\"\n",
    "    expand english contractions\n",
    "    \"\"\"\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "            if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def autospell(text):\n",
    "    \"\"\"\n",
    "    correct the spelling of the word.\n",
    "    \"\"\"\n",
    "    spell = Speller(lang='en',fast=True)\n",
    "    spells = [spell(w) for w in (nltk.word_tokenize(text))]\n",
    "    return \" \".join(spells)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner']) #load small english core library\n",
    "stops = stopwords.words(\"english\") #load stopwords\n",
    "\n",
    "\n",
    "\n",
    "def normalize(text, lowercase, remove_stopwords):\n",
    "    '''\n",
    "    clean the text into desired format for text analysis\n",
    "    '''\n",
    "    text = expand_contractions(text,contractions_dict) #expand english contractions\n",
    "    text = autospell(text) #correct spelling\n",
    "    #text = ' '.join([w.lower() for w in nltk.word_tokenize(text)])  #lowercase\n",
    "    text = re.sub('<[^<]+?>','', text)  #remove brackets\n",
    "    text = ''.join(c for c in text if not c.isdigit())  #remove numbers\n",
    "    text = ''.join(c for c in text if c not in punct)  #remove punctuations\n",
    "    if lowercase:\n",
    "        text = text.lower()  #lowercase\n",
    "    text = nlp(text)\n",
    "    lemmatized = list()\n",
    "    for word in text:\n",
    "        lemma = word.lemma_.strip() #tokenize\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):  #remove stopwords\n",
    "                lemmatized.append(lemma)\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Hot100Data3483.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df['lang']=df.Lyrics.apply(detect)\n",
    "df=df[df['lang']=='en']\n",
    "df['After_Clean'] = df['Lyrics'].apply(normalize, lowercase=True, remove_stopwords=True)\n",
    "df.to_csv('cleaned3000.csv')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}